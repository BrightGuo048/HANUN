{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention U-shaped Pyramid Segmentation Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import torch.nn.functional as F\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as standard_transforms\n",
    "\n",
    "import numpy as np\n",
    "import glob\n",
    "\n",
    "from data_loader import Rescale\n",
    "from data_loader import RescaleT\n",
    "from data_loader import RandomCrop\n",
    "from data_loader import ToTensor\n",
    "from data_loader import ToTensorLab\n",
    "from data_loader import SalObjDataset\n",
    "\n",
    "from model import U2NET\n",
    "from model import U2NETP\n",
    "\n",
    "from model import UPSPNet\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use('AGG')\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import eval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## set parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_num = 500\n",
    "batch_size_train = 36\n",
    "batch_size_val = 1\n",
    "train_num = 0\n",
    "val_num = 0\n",
    "\n",
    "ite_num = 0\n",
    "running_loss = 0.0\n",
    "running_tar_loss = 0.0\n",
    "ite_num4val = 0\n",
    "save_epoch = 5\n",
    "Loss_list = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## define loss function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "bce_loss = nn.BCELoss(size_average=True)\n",
    "\n",
    "def muti_bce_loss_fusion(d0, d1, d2, d3, d4, d5, d6, labels_v):\n",
    "\n",
    "    loss0 = bce_loss(d0,labels_v)\n",
    "    loss1 = bce_loss(d1,labels_v)\n",
    "    loss2 = bce_loss(d2,labels_v)\n",
    "    loss3 = bce_loss(d3,labels_v)\n",
    "    loss4 = bce_loss(d4,labels_v)\n",
    "    loss5 = bce_loss(d5,labels_v)\n",
    "    loss6 = bce_loss(d6,labels_v)\n",
    "\n",
    "    loss = loss0 + loss1 + loss2 + loss3 + loss4 + loss5 + loss6\n",
    "    print(\"l0: %3f, l1: %3f, l2: %3f, l3: %3f, l4: %3f, l5: %3f, l6: %3f\\n\"%(loss0.data,loss1.data,loss2.data,loss3.data,loss4.data,loss5.data,loss6.data))\n",
    "\n",
    "    return loss0, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## set the directory of training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'upspnet' #'u2netp'\n",
    "\n",
    "data_dir = os.path.join(os.getcwd(), '/mnt/DATA_512/Train' + os.sep)\n",
    "tra_image_dir = os.path.join('src' + os.sep)\n",
    "tra_label_dir = os.path.join('gt' + os.sep)\n",
    "\n",
    "image_ext = '.jpg'\n",
    "label_ext = '.png'\n",
    "\n",
    "model_dir = os.path.join(os.getcwd(), 'saved_models', \"auspnet_512\" + os.sep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "train images:  4800\n",
      "train labels:  4800\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "tra_img_name_list = glob.glob(data_dir + tra_image_dir + '*' + image_ext)\n",
    "\n",
    "tra_lbl_name_list = []\n",
    "for img_path in tra_img_name_list:\n",
    "\timg_name = img_path.split(os.sep)[-1]\n",
    "\n",
    "\taaa = img_name.split(\".\")\n",
    "\tbbb = aaa[0:-1]\n",
    "\timidx = bbb[0]\n",
    "\tfor i in range(1,len(bbb)):\n",
    "\t\timidx = imidx + \".\" + bbb[i]\n",
    "\n",
    "\ttra_lbl_name_list.append(data_dir + tra_label_dir + imidx + label_ext)\n",
    "\n",
    "print(\"---\")\n",
    "print(\"train images: \", len(tra_img_name_list))\n",
    "print(\"train labels: \", len(tra_lbl_name_list))\n",
    "print(\"---\")\n",
    "\n",
    "train_num = len(tra_img_name_list)\n",
    "\n",
    "salobj_dataset = SalObjDataset(\n",
    "    img_name_list=tra_img_name_list,\n",
    "    lbl_name_list=tra_lbl_name_list,\n",
    "    transform=transforms.Compose([\n",
    "        RescaleT(320),\n",
    "        RandomCrop(288),\n",
    "        ToTensorLab(flag=0)]))\n",
    "salobj_dataloader = DataLoader(salobj_dataset, batch_size=batch_size_train, shuffle=True, num_workers=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = UPSPNet.UPSPNET_RSU(3, 1)\n",
    "#net=torch.nn.DataParallel(net)\n",
    "#net = nn.DataParallel(net) # multi-GPU\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    net.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## define optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(net.parameters(), lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---start training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/myconda/lib/python3.8/site-packages/torch/nn/functional.py:2952: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\n",
      "  warnings.warn(\"nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\")\n",
      "/root/miniconda3/envs/myconda/lib/python3.8/site-packages/torch/nn/functional.py:3060: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
      "  warnings.warn(\"Default upsampling behavior when mode={} is changed \"\n",
      "/root/miniconda3/envs/myconda/lib/python3.8/site-packages/torch/nn/functional.py:1639: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "l0: 0.671697, l1: 0.790183, l2: 0.687988, l3: 0.734891, l4: 0.687826, l5: 0.736316, l6: 0.711290\n",
      "\n",
      "[epoch:   1/500, batch:    36/ 4800, ite: 1] train loss: 5.020191, tar: 0.671697 \n",
      "l0: 0.765791, l1: 0.683958, l2: 0.703052, l3: 0.697804, l4: 0.917278, l5: 1.347346, l6: 1.187292\n",
      "\n",
      "[epoch:   1/500, batch:    72/ 4800, ite: 2] train loss: 5.661357, tar: 0.718744 \n",
      "l0: 0.629138, l1: 0.706826, l2: 0.706599, l3: 0.740176, l4: 0.762037, l5: 1.149707, l6: 0.662870\n",
      "\n",
      "[epoch:   1/500, batch:   108/ 4800, ite: 3] train loss: 5.560022, tar: 0.688876 \n",
      "l0: 0.721754, l1: 0.671377, l2: 0.670369, l3: 0.688932, l4: 0.903362, l5: 1.065774, l6: 0.776250\n",
      "\n",
      "[epoch:   1/500, batch:   144/ 4800, ite: 4] train loss: 5.544471, tar: 0.697095 \n",
      "l0: 0.716175, l1: 0.624220, l2: 0.639525, l3: 0.654826, l4: 0.695071, l5: 0.704586, l6: 0.879921\n",
      "\n",
      "[epoch:   1/500, batch:   180/ 4800, ite: 5] train loss: 5.418441, tar: 0.700911 \n",
      "l0: 0.660246, l1: 0.619649, l2: 0.637464, l3: 0.640643, l4: 0.671824, l5: 0.731769, l6: 0.647085\n",
      "\n",
      "[epoch:   1/500, batch:   216/ 4800, ite: 6] train loss: 5.283481, tar: 0.694133 \n",
      "l0: 0.664684, l1: 0.632870, l2: 0.645700, l3: 0.651626, l4: 0.673755, l5: 0.670517, l6: 0.709204\n",
      "\n",
      "[epoch:   1/500, batch:   252/ 4800, ite: 7] train loss: 5.192749, tar: 0.689926 \n",
      "l0: 0.652138, l1: 0.647923, l2: 0.661246, l3: 0.665251, l4: 0.702751, l5: 0.742883, l6: 0.689952\n",
      "\n",
      "[epoch:   1/500, batch:   288/ 4800, ite: 8] train loss: 5.138924, tar: 0.685203 \n",
      "l0: 0.657844, l1: 0.656975, l2: 0.656952, l3: 0.653597, l4: 0.681980, l5: 0.672305, l6: 0.686675\n",
      "\n",
      "[epoch:   1/500, batch:   324/ 4800, ite: 9] train loss: 5.086413, tar: 0.682163 \n",
      "l0: 0.631985, l1: 0.608105, l2: 0.606624, l3: 0.615233, l4: 0.610944, l5: 0.613348, l6: 0.605139\n",
      "\n",
      "[epoch:   1/500, batch:   360/ 4800, ite: 10] train loss: 5.006910, tar: 0.677145 \n",
      "l0: 0.628747, l1: 0.602899, l2: 0.604746, l3: 0.608400, l4: 0.617218, l5: 0.619782, l6: 0.598059\n",
      "\n",
      "[epoch:   1/500, batch:   396/ 4800, ite: 11] train loss: 4.940814, tar: 0.672745 \n",
      "l0: 0.657276, l1: 0.651257, l2: 0.655677, l3: 0.654848, l4: 0.715362, l5: 0.733067, l6: 0.655760\n",
      "\n",
      "[epoch:   1/500, batch:   432/ 4800, ite: 12] train loss: 4.922683, tar: 0.671456 \n",
      "l0: 0.650186, l1: 0.625217, l2: 0.630008, l3: 0.637835, l4: 0.693682, l5: 0.689039, l6: 0.647292\n",
      "\n",
      "[epoch:   1/500, batch:   468/ 4800, ite: 13] train loss: 4.895804, tar: 0.669820 \n",
      "l0: 0.640713, l1: 0.624491, l2: 0.635496, l3: 0.647588, l4: 0.659838, l5: 0.678001, l6: 0.642838\n",
      "\n",
      "[epoch:   1/500, batch:   504/ 4800, ite: 14] train loss: 4.869601, tar: 0.667741 \n",
      "l0: 0.678986, l1: 0.642316, l2: 0.673193, l3: 0.702016, l4: 0.712338, l5: 0.788661, l6: 0.721166\n",
      "\n",
      "[epoch:   1/500, batch:   540/ 4800, ite: 15] train loss: 4.872873, tar: 0.668491 \n",
      "l0: 0.647094, l1: 0.633999, l2: 0.638255, l3: 0.641537, l4: 0.658585, l5: 0.660859, l6: 0.664806\n",
      "\n",
      "[epoch:   1/500, batch:   576/ 4800, ite: 16] train loss: 4.852389, tar: 0.667153 \n",
      "l0: 0.636615, l1: 0.636777, l2: 0.634777, l3: 0.629961, l4: 0.647392, l5: 0.612484, l6: 0.636304\n",
      "\n",
      "[epoch:   1/500, batch:   612/ 4800, ite: 17] train loss: 4.827796, tar: 0.665357 \n",
      "l0: 0.656543, l1: 0.635720, l2: 0.640817, l3: 0.648359, l4: 0.663617, l5: 0.677618, l6: 0.666649\n",
      "\n",
      "[epoch:   1/500, batch:   648/ 4800, ite: 18] train loss: 4.814548, tar: 0.664867 \n",
      "l0: 0.659026, l1: 0.632968, l2: 0.641269, l3: 0.649816, l4: 0.652965, l5: 0.664856, l6: 0.655811\n",
      "\n",
      "[epoch:   1/500, batch:   684/ 4800, ite: 19] train loss: 4.800978, tar: 0.664560 \n",
      "l0: 0.650650, l1: 0.618037, l2: 0.628995, l3: 0.637110, l4: 0.624587, l5: 0.626774, l6: 0.632403\n",
      "\n",
      "[epoch:   1/500, batch:   720/ 4800, ite: 20] train loss: 4.781857, tar: 0.663864 \n",
      "l0: 0.658018, l1: 0.607149, l2: 0.624487, l3: 0.634218, l4: 0.630024, l5: 0.704774, l6: 0.655509\n",
      "\n",
      "[epoch:   1/500, batch:   756/ 4800, ite: 21] train loss: 4.769110, tar: 0.663586 \n",
      "l0: 0.667355, l1: 0.652151, l2: 0.651727, l3: 0.659143, l4: 0.664655, l5: 0.695760, l6: 0.670690\n",
      "\n",
      "[epoch:   1/500, batch:   792/ 4800, ite: 22] train loss: 4.764218, tar: 0.663757 \n",
      "l0: 0.662533, l1: 0.621896, l2: 0.630437, l3: 0.628785, l4: 0.663271, l5: 0.663161, l6: 0.666264\n",
      "\n",
      "[epoch:   1/500, batch:   828/ 4800, ite: 23] train loss: 4.754311, tar: 0.663704 \n",
      "l0: 0.652095, l1: 0.610147, l2: 0.615247, l3: 0.624441, l4: 0.638166, l5: 0.664321, l6: 0.648099\n",
      "\n",
      "[epoch:   1/500, batch:   864/ 4800, ite: 24] train loss: 4.741736, tar: 0.663220 \n",
      "l0: 0.669381, l1: 0.657772, l2: 0.662757, l3: 0.665183, l4: 0.671972, l5: 0.658625, l6: 0.680578\n",
      "\n",
      "[epoch:   1/500, batch:   900/ 4800, ite: 25] train loss: 4.738717, tar: 0.663467 \n",
      "l0: 0.694761, l1: 0.676265, l2: 0.688799, l3: 0.691701, l4: 0.741612, l5: 0.726099, l6: 0.733116\n",
      "\n",
      "[epoch:   1/500, batch:   936/ 4800, ite: 26] train loss: 4.746934, tar: 0.664670 \n",
      "l0: 0.631619, l1: 0.579368, l2: 0.587492, l3: 0.601719, l4: 0.611435, l5: 0.607847, l6: 0.615256\n",
      "\n",
      "[epoch:   1/500, batch:   972/ 4800, ite: 27] train loss: 4.727963, tar: 0.663446 \n",
      "l0: 0.644000, l1: 0.623157, l2: 0.616742, l3: 0.619308, l4: 0.634347, l5: 0.628823, l6: 0.636513\n",
      "\n",
      "[epoch:   1/500, batch:  1008/ 4800, ite: 28] train loss: 4.716354, tar: 0.662752 \n",
      "l0: 0.655062, l1: 0.649420, l2: 0.640893, l3: 0.645429, l4: 0.655310, l5: 0.662644, l6: 0.659965\n",
      "\n",
      "[epoch:   1/500, batch:  1044/ 4800, ite: 29] train loss: 4.711263, tar: 0.662487 \n",
      "l0: 0.655494, l1: 0.623967, l2: 0.622396, l3: 0.635634, l4: 0.670935, l5: 0.667902, l6: 0.649887\n",
      "\n",
      "[epoch:   1/500, batch:  1080/ 4800, ite: 30] train loss: 4.705095, tar: 0.662254 \n",
      "l0: 0.668715, l1: 0.654892, l2: 0.658478, l3: 0.667395, l4: 0.689023, l5: 0.682418, l6: 0.691319\n",
      "\n",
      "[epoch:   1/500, batch:  1116/ 4800, ite: 31] train loss: 4.705325, tar: 0.662462 \n",
      "l0: 0.636779, l1: 0.602755, l2: 0.615938, l3: 0.602914, l4: 0.658449, l5: 0.618022, l6: 0.614278\n",
      "\n",
      "[epoch:   1/500, batch:  1152/ 4800, ite: 32] train loss: 4.694194, tar: 0.661659 \n",
      "l0: 0.644643, l1: 0.618222, l2: 0.628683, l3: 0.616633, l4: 0.637144, l5: 0.637562, l6: 0.646625\n",
      "\n",
      "[epoch:   1/500, batch:  1188/ 4800, ite: 33] train loss: 4.686174, tar: 0.661144 \n",
      "l0: 0.653000, l1: 0.653373, l2: 0.654294, l3: 0.663301, l4: 0.682443, l5: 0.668725, l6: 0.655404\n",
      "\n",
      "[epoch:   1/500, batch:  1224/ 4800, ite: 34] train loss: 4.684537, tar: 0.660904 \n",
      "l0: 0.630189, l1: 0.612935, l2: 0.598362, l3: 0.619369, l4: 0.659290, l5: 0.636719, l6: 0.623552\n",
      "\n",
      "[epoch:   1/500, batch:  1260/ 4800, ite: 35] train loss: 4.675848, tar: 0.660027 \n",
      "l0: 0.656815, l1: 0.634645, l2: 0.618407, l3: 0.640816, l4: 0.704519, l5: 0.625392, l6: 0.641455\n",
      "\n",
      "[epoch:   1/500, batch:  1296/ 4800, ite: 36] train loss: 4.671576, tar: 0.659937 \n",
      "l0: 0.639160, l1: 0.611196, l2: 0.613154, l3: 0.625595, l4: 0.656106, l5: 0.676838, l6: 0.654141\n",
      "\n",
      "[epoch:   1/500, batch:  1332/ 4800, ite: 37] train loss: 4.666296, tar: 0.659376 \n",
      "l0: 0.617592, l1: 0.588912, l2: 0.571778, l3: 0.582405, l4: 0.597440, l5: 0.654927, l6: 0.624708\n",
      "\n",
      "[epoch:   1/500, batch:  1368/ 4800, ite: 38] train loss: 4.655018, tar: 0.658276 \n",
      "l0: 0.622725, l1: 0.588558, l2: 0.585594, l3: 0.591442, l4: 0.604606, l5: 0.618231, l6: 0.631209\n",
      "\n",
      "[epoch:   1/500, batch:  1404/ 4800, ite: 39] train loss: 4.644438, tar: 0.657365 \n",
      "l0: 0.597325, l1: 0.561626, l2: 0.547384, l3: 0.570235, l4: 0.589311, l5: 0.586569, l6: 0.587103\n",
      "\n",
      "[epoch:   1/500, batch:  1440/ 4800, ite: 40] train loss: 4.629316, tar: 0.655864 \n",
      "l0: 0.623154, l1: 0.584110, l2: 0.584867, l3: 0.580953, l4: 0.607558, l5: 0.670815, l6: 0.686218\n",
      "\n",
      "[epoch:   1/500, batch:  1476/ 4800, ite: 41] train loss: 4.622202, tar: 0.655066 \n",
      "l0: 0.618775, l1: 0.611861, l2: 0.603679, l3: 0.612160, l4: 0.645403, l5: 0.620259, l6: 0.643395\n",
      "\n",
      "[epoch:   1/500, batch:  1512/ 4800, ite: 42] train loss: 4.615853, tar: 0.654202 \n",
      "l0: 0.610656, l1: 0.610398, l2: 0.607307, l3: 0.614471, l4: 0.620896, l5: 0.608062, l6: 0.614442\n",
      "\n",
      "[epoch:   1/500, batch:  1548/ 4800, ite: 43] train loss: 4.608187, tar: 0.653189 \n",
      "l0: 0.634701, l1: 0.607189, l2: 0.624035, l3: 0.629834, l4: 0.639577, l5: 0.664508, l6: 0.685530\n",
      "\n",
      "[epoch:   1/500, batch:  1584/ 4800, ite: 44] train loss: 4.605396, tar: 0.652769 \n",
      "l0: 0.614639, l1: 0.593075, l2: 0.595580, l3: 0.601067, l4: 0.614810, l5: 0.641151, l6: 0.662548\n",
      "\n",
      "[epoch:   1/500, batch:  1620/ 4800, ite: 45] train loss: 4.599118, tar: 0.651922 \n",
      "l0: 0.600365, l1: 0.565364, l2: 0.574574, l3: 0.582308, l4: 0.601943, l5: 0.590366, l6: 0.594668\n",
      "\n",
      "[epoch:   1/500, batch:  1656/ 4800, ite: 46] train loss: 4.588476, tar: 0.650801 \n",
      "l0: 0.558910, l1: 0.545139, l2: 0.545191, l3: 0.529393, l4: 0.518937, l5: 0.537654, l6: 0.537664\n",
      "\n",
      "[epoch:   1/500, batch:  1692/ 4800, ite: 47] train loss: 4.571123, tar: 0.648846 \n",
      "l0: 0.628024, l1: 0.604069, l2: 0.599276, l3: 0.626570, l4: 0.661155, l5: 0.680688, l6: 0.682753\n",
      "\n",
      "[epoch:   1/500, batch:  1728/ 4800, ite: 48] train loss: 4.569278, tar: 0.648412 \n",
      "l0: 0.592981, l1: 0.585480, l2: 0.577702, l3: 0.570382, l4: 0.590829, l5: 0.620642, l6: 0.633021\n",
      "\n",
      "[epoch:   1/500, batch:  1764/ 4800, ite: 49] train loss: 4.561150, tar: 0.647281 \n",
      "l0: 0.555266, l1: 0.563873, l2: 0.534076, l3: 0.538179, l4: 0.548204, l5: 0.578037, l6: 0.548898\n",
      "\n",
      "[epoch:   1/500, batch:  1800/ 4800, ite: 50] train loss: 4.547257, tar: 0.645440 \n",
      "l0: 0.590024, l1: 0.598133, l2: 0.589457, l3: 0.581988, l4: 0.594180, l5: 0.596124, l6: 0.598978\n",
      "\n",
      "[epoch:   1/500, batch:  1836/ 4800, ite: 51] train loss: 4.539446, tar: 0.644354 \n",
      "l0: 0.575713, l1: 0.555125, l2: 0.555616, l3: 0.565546, l4: 0.579527, l5: 0.595308, l6: 0.594807\n",
      "\n",
      "[epoch:   1/500, batch:  1872/ 4800, ite: 52] train loss: 4.529489, tar: 0.643034 \n",
      "l0: 0.579018, l1: 0.560972, l2: 0.560030, l3: 0.571805, l4: 0.599416, l5: 0.622756, l6: 0.613948\n",
      "\n",
      "[epoch:   1/500, batch:  1908/ 4800, ite: 53] train loss: 4.521535, tar: 0.641826 \n",
      "l0: 0.532579, l1: 0.530776, l2: 0.519150, l3: 0.520857, l4: 0.523542, l5: 0.522399, l6: 0.519662\n",
      "\n",
      "[epoch:   1/500, batch:  1944/ 4800, ite: 54] train loss: 4.505746, tar: 0.639803 \n",
      "l0: 0.618272, l1: 0.618992, l2: 0.649735, l3: 0.634440, l4: 0.622472, l5: 0.634709, l6: 0.674872\n",
      "\n",
      "[epoch:   1/500, batch:  1980/ 4800, ite: 55] train loss: 4.504796, tar: 0.639411 \n",
      "l0: 0.598237, l1: 0.582775, l2: 0.594033, l3: 0.585402, l4: 0.593851, l5: 0.614055, l6: 0.637212\n",
      "\n",
      "[epoch:   1/500, batch:  2016/ 4800, ite: 56] train loss: 4.499453, tar: 0.638676 \n",
      "l0: 0.618779, l1: 0.631347, l2: 0.634979, l3: 0.626454, l4: 0.633838, l5: 0.644849, l6: 0.654214\n",
      "\n",
      "[epoch:   1/500, batch:  2052/ 4800, ite: 57] train loss: 4.498488, tar: 0.638327 \n",
      "l0: 0.586300, l1: 0.587423, l2: 0.587087, l3: 0.583345, l4: 0.576765, l5: 0.575388, l6: 0.576371\n",
      "\n",
      "[epoch:   1/500, batch:  2088/ 4800, ite: 58] train loss: 4.491147, tar: 0.637430 \n",
      "l0: 0.600281, l1: 0.596454, l2: 0.595762, l3: 0.591291, l4: 0.588193, l5: 0.606320, l6: 0.610475\n",
      "\n",
      "[epoch:   1/500, batch:  2124/ 4800, ite: 59] train loss: 4.486022, tar: 0.636800 \n",
      "l0: 0.589375, l1: 0.579820, l2: 0.573300, l3: 0.565057, l4: 0.574076, l5: 0.586300, l6: 0.590525\n",
      "\n",
      "[epoch:   1/500, batch:  2160/ 4800, ite: 60] train loss: 4.478896, tar: 0.636010 \n",
      "l0: 0.595885, l1: 0.578211, l2: 0.580307, l3: 0.580204, l4: 0.583215, l5: 0.595091, l6: 0.601221\n",
      "\n",
      "[epoch:   1/500, batch:  2196/ 4800, ite: 61] train loss: 4.472916, tar: 0.635352 \n",
      "l0: 0.565974, l1: 0.550295, l2: 0.547509, l3: 0.543437, l4: 0.541938, l5: 0.556516, l6: 0.554641\n",
      "\n",
      "[epoch:   1/500, batch:  2232/ 4800, ite: 62] train loss: 4.463035, tar: 0.634233 \n",
      "l0: 0.599801, l1: 0.586608, l2: 0.588811, l3: 0.580366, l4: 0.566030, l5: 0.593812, l6: 0.619091\n",
      "\n",
      "[epoch:   1/500, batch:  2268/ 4800, ite: 63] train loss: 4.457821, tar: 0.633687 \n",
      "l0: 0.575314, l1: 0.553310, l2: 0.560543, l3: 0.562199, l4: 0.576713, l5: 0.580136, l6: 0.572923\n",
      "\n",
      "[epoch:   1/500, batch:  2304/ 4800, ite: 64] train loss: 4.450373, tar: 0.632774 \n",
      "l0: 0.566700, l1: 0.560020, l2: 0.556444, l3: 0.556209, l4: 0.559107, l5: 0.562997, l6: 0.571711\n",
      "\n",
      "[epoch:   1/500, batch:  2340/ 4800, ite: 65] train loss: 4.442416, tar: 0.631758 \n",
      "l0: 0.611403, l1: 0.596018, l2: 0.612128, l3: 0.612477, l4: 0.627388, l5: 0.608347, l6: 0.615025\n",
      "\n",
      "[epoch:   1/500, batch:  2376/ 4800, ite: 66] train loss: 4.439998, tar: 0.631450 \n",
      "l0: 0.572594, l1: 0.574455, l2: 0.581203, l3: 0.573046, l4: 0.577692, l5: 0.568002, l6: 0.583950\n",
      "\n",
      "[epoch:   1/500, batch:  2412/ 4800, ite: 67] train loss: 4.433892, tar: 0.630571 \n",
      "l0: 0.572462, l1: 0.560895, l2: 0.564797, l3: 0.565687, l4: 0.593082, l5: 0.583525, l6: 0.579780\n",
      "\n",
      "[epoch:   1/500, batch:  2448/ 4800, ite: 68] train loss: 4.427809, tar: 0.629717 \n",
      "l0: 0.543681, l1: 0.535721, l2: 0.540363, l3: 0.542501, l4: 0.546816, l5: 0.537571, l6: 0.553652\n",
      "\n",
      "[epoch:   1/500, batch:  2484/ 4800, ite: 69] train loss: 4.418715, tar: 0.628470 \n",
      "l0: 0.604297, l1: 0.609275, l2: 0.630722, l3: 0.625163, l4: 0.626649, l5: 0.619375, l6: 0.622346\n",
      "\n",
      "[epoch:   1/500, batch:  2520/ 4800, ite: 70] train loss: 4.417559, tar: 0.628124 \n",
      "l0: 0.569279, l1: 0.573981, l2: 0.588200, l3: 0.589920, l4: 0.589779, l5: 0.556363, l6: 0.557487\n",
      "\n",
      "[epoch:   1/500, batch:  2556/ 4800, ite: 71] train loss: 4.412030, tar: 0.627295 \n",
      "l0: 0.547434, l1: 0.549669, l2: 0.559872, l3: 0.553385, l4: 0.561468, l5: 0.541475, l6: 0.539218\n",
      "\n",
      "[epoch:   1/500, batch:  2592/ 4800, ite: 72] train loss: 4.404259, tar: 0.626186 \n",
      "l0: 0.511037, l1: 0.495803, l2: 0.488756, l3: 0.483427, l4: 0.496260, l5: 0.490514, l6: 0.486959\n",
      "\n",
      "[epoch:   1/500, batch:  2628/ 4800, ite: 73] train loss: 4.391225, tar: 0.624609 \n",
      "l0: 0.518797, l1: 0.514175, l2: 0.516804, l3: 0.511928, l4: 0.502089, l5: 0.503456, l6: 0.510712\n",
      "\n",
      "[epoch:   1/500, batch:  2664/ 4800, ite: 74] train loss: 4.380235, tar: 0.623179 \n",
      "l0: 0.562374, l1: 0.539691, l2: 0.537398, l3: 0.552042, l4: 0.587850, l5: 0.579683, l6: 0.568612\n",
      "\n",
      "[epoch:   1/500, batch:  2700/ 4800, ite: 75] train loss: 4.374200, tar: 0.622368 \n",
      "l0: 0.518995, l1: 0.511528, l2: 0.519843, l3: 0.519404, l4: 0.533720, l5: 0.508395, l6: 0.512915\n",
      "\n",
      "[epoch:   1/500, batch:  2736/ 4800, ite: 76] train loss: 4.364340, tar: 0.621008 \n",
      "l0: 0.535298, l1: 0.535905, l2: 0.547514, l3: 0.552261, l4: 0.553508, l5: 0.526942, l6: 0.539711\n",
      "\n",
      "[epoch:   1/500, batch:  2772/ 4800, ite: 77] train loss: 4.356895, tar: 0.619895 \n",
      "l0: 0.562299, l1: 0.551788, l2: 0.570126, l3: 0.566069, l4: 0.570794, l5: 0.601645, l6: 0.633000\n",
      "\n",
      "[epoch:   1/500, batch:  2808/ 4800, ite: 78] train loss: 4.353034, tar: 0.619157 \n",
      "l0: 0.590671, l1: 0.605163, l2: 0.606540, l3: 0.614305, l4: 0.597057, l5: 0.584124, l6: 0.631057\n",
      "\n",
      "[epoch:   1/500, batch:  2844/ 4800, ite: 79] train loss: 4.351463, tar: 0.618796 \n",
      "l0: 0.514293, l1: 0.489233, l2: 0.496100, l3: 0.499813, l4: 0.507333, l5: 0.501155, l6: 0.505161\n",
      "\n",
      "[epoch:   1/500, batch:  2880/ 4800, ite: 80] train loss: 4.340984, tar: 0.617490 \n",
      "l0: 0.539984, l1: 0.522765, l2: 0.516259, l3: 0.519608, l4: 0.546730, l5: 0.549867, l6: 0.561467\n",
      "\n",
      "[epoch:   1/500, batch:  2916/ 4800, ite: 81] train loss: 4.333770, tar: 0.616533 \n",
      "l0: 0.511840, l1: 0.494359, l2: 0.500535, l3: 0.507450, l4: 0.516219, l5: 0.485772, l6: 0.499766\n",
      "\n",
      "[epoch:   1/500, batch:  2952/ 4800, ite: 82] train loss: 4.323796, tar: 0.615256 \n",
      "l0: 0.513807, l1: 0.501136, l2: 0.503961, l3: 0.514121, l4: 0.532075, l5: 0.508854, l6: 0.503954\n",
      "\n",
      "[epoch:   1/500, batch:  2988/ 4800, ite: 83] train loss: 4.314810, tar: 0.614034 \n",
      "l0: 0.527835, l1: 0.523391, l2: 0.521338, l3: 0.527506, l4: 0.542280, l5: 0.526683, l6: 0.536548\n",
      "\n",
      "[epoch:   1/500, batch:  3024/ 4800, ite: 84] train loss: 4.307557, tar: 0.613008 \n",
      "l0: 0.482071, l1: 0.473078, l2: 0.467938, l3: 0.469206, l4: 0.474260, l5: 0.464147, l6: 0.487242\n",
      "\n",
      "[epoch:   1/500, batch:  3060/ 4800, ite: 85] train loss: 4.295914, tar: 0.611467 \n",
      "l0: 0.488168, l1: 0.493849, l2: 0.491164, l3: 0.498842, l4: 0.509368, l5: 0.489255, l6: 0.477833\n",
      "\n",
      "[epoch:   1/500, batch:  3096/ 4800, ite: 86] train loss: 4.286060, tar: 0.610034 \n",
      "l0: 0.588616, l1: 0.568688, l2: 0.581029, l3: 0.581248, l4: 0.627738, l5: 0.622386, l6: 0.616470\n",
      "\n",
      "[epoch:   1/500, batch:  3132/ 4800, ite: 87] train loss: 4.284913, tar: 0.609787 \n",
      "l0: 0.529683, l1: 0.516671, l2: 0.521850, l3: 0.528458, l4: 0.574034, l5: 0.537979, l6: 0.527871\n",
      "\n",
      "[epoch:   1/500, batch:  3168/ 4800, ite: 88] train loss: 4.278681, tar: 0.608877 \n",
      "l0: 0.470599, l1: 0.462614, l2: 0.455578, l3: 0.463058, l4: 0.473802, l5: 0.468889, l6: 0.487550\n",
      "\n",
      "[epoch:   1/500, batch:  3204/ 4800, ite: 89] train loss: 4.267483, tar: 0.607323 \n",
      "l0: 0.508277, l1: 0.512417, l2: 0.524854, l3: 0.534760, l4: 0.598064, l5: 0.502276, l6: 0.517491\n",
      "\n",
      "[epoch:   1/500, batch:  3240/ 4800, ite: 90] train loss: 4.261158, tar: 0.606223 \n",
      "l0: 0.555205, l1: 0.553850, l2: 0.574142, l3: 0.577698, l4: 0.599749, l5: 0.557760, l6: 0.547829\n",
      "\n",
      "[epoch:   1/500, batch:  3276/ 4800, ite: 91] train loss: 4.257916, tar: 0.605662 \n",
      "l0: 0.498785, l1: 0.489842, l2: 0.492826, l3: 0.499435, l4: 0.502503, l5: 0.541979, l6: 0.527499\n",
      "\n",
      "[epoch:   1/500, batch:  3312/ 4800, ite: 92] train loss: 4.250253, tar: 0.604501 \n",
      "l0: 0.577707, l1: 0.574475, l2: 0.604913, l3: 0.596024, l4: 0.583414, l5: 0.582525, l6: 0.614893\n",
      "\n",
      "[epoch:   1/500, batch:  3348/ 4800, ite: 93] train loss: 4.249002, tar: 0.604212 \n",
      "l0: 0.570366, l1: 0.571860, l2: 0.594548, l3: 0.579788, l4: 0.578132, l5: 0.577237, l6: 0.591902\n",
      "\n",
      "[epoch:   1/500, batch:  3384/ 4800, ite: 94] train loss: 4.247032, tar: 0.603852 \n",
      "l0: 0.569519, l1: 0.573459, l2: 0.572010, l3: 0.567613, l4: 0.558999, l5: 0.587094, l6: 0.592241\n",
      "\n",
      "[epoch:   1/500, batch:  3420/ 4800, ite: 95] train loss: 4.244652, tar: 0.603491 \n",
      "l0: 0.570105, l1: 0.555111, l2: 0.560582, l3: 0.565955, l4: 0.588511, l5: 0.572990, l6: 0.571517\n",
      "\n",
      "[epoch:   1/500, batch:  3456/ 4800, ite: 96] train loss: 4.241945, tar: 0.603143 \n",
      "l0: 0.544010, l1: 0.511699, l2: 0.515287, l3: 0.528297, l4: 0.559007, l5: 0.526668, l6: 0.537108\n",
      "\n",
      "[epoch:   1/500, batch:  3492/ 4800, ite: 97] train loss: 4.236585, tar: 0.602534 \n",
      "l0: 0.499244, l1: 0.481588, l2: 0.468794, l3: 0.477790, l4: 0.502791, l5: 0.480321, l6: 0.492192\n",
      "\n",
      "[epoch:   1/500, batch:  3528/ 4800, ite: 98] train loss: 4.228076, tar: 0.601480 \n",
      "l0: 0.547689, l1: 0.566333, l2: 0.563147, l3: 0.558455, l4: 0.576428, l5: 0.565029, l6: 0.556130\n",
      "\n",
      "[epoch:   1/500, batch:  3564/ 4800, ite: 99] train loss: 4.225098, tar: 0.600936 \n",
      "l0: 0.520676, l1: 0.528638, l2: 0.525163, l3: 0.516338, l4: 0.523832, l5: 0.514195, l6: 0.528365\n",
      "\n",
      "[epoch:   1/500, batch:  3600/ 4800, ite: 100] train loss: 4.219419, tar: 0.600134 \n",
      "l0: 0.513133, l1: 0.520329, l2: 0.519745, l3: 0.519061, l4: 0.512297, l5: 0.508190, l6: 0.520987\n",
      "\n",
      "[epoch:   1/500, batch:  3636/ 4800, ite: 101] train loss: 4.213422, tar: 0.599272 \n",
      "l0: 0.500769, l1: 0.498965, l2: 0.499771, l3: 0.498017, l4: 0.511129, l5: 0.508355, l6: 0.503423\n",
      "\n",
      "[epoch:   1/500, batch:  3672/ 4800, ite: 102] train loss: 4.206628, tar: 0.598307 \n",
      "l0: 0.457416, l1: 0.460105, l2: 0.462529, l3: 0.462314, l4: 0.452813, l5: 0.451834, l6: 0.456842\n",
      "\n",
      "[epoch:   1/500, batch:  3708/ 4800, ite: 103] train loss: 4.196892, tar: 0.596939 \n",
      "l0: 0.540003, l1: 0.534081, l2: 0.540393, l3: 0.542466, l4: 0.569191, l5: 0.552085, l6: 0.564667\n",
      "\n",
      "[epoch:   1/500, batch:  3744/ 4800, ite: 104] train loss: 4.193489, tar: 0.596391 \n",
      "l0: 0.484866, l1: 0.488942, l2: 0.481145, l3: 0.487780, l4: 0.524779, l5: 0.499410, l6: 0.505939\n",
      "\n",
      "[epoch:   1/500, batch:  3780/ 4800, ite: 105] train loss: 4.186625, tar: 0.595329 \n",
      "l0: 0.479235, l1: 0.481563, l2: 0.475334, l3: 0.488192, l4: 0.507246, l5: 0.474553, l6: 0.483017\n",
      "\n",
      "[epoch:   1/500, batch:  3816/ 4800, ite: 106] train loss: 4.179102, tar: 0.594234 \n",
      "l0: 0.466119, l1: 0.456847, l2: 0.452326, l3: 0.456393, l4: 0.467483, l5: 0.472728, l6: 0.500344\n",
      "\n",
      "[epoch:   1/500, batch:  3852/ 4800, ite: 107] train loss: 4.170627, tar: 0.593037 \n",
      "l0: 0.438905, l1: 0.427200, l2: 0.427715, l3: 0.429773, l4: 0.440309, l5: 0.437818, l6: 0.442999\n",
      "\n",
      "[epoch:   1/500, batch:  3888/ 4800, ite: 108] train loss: 4.160202, tar: 0.591609 \n",
      "l0: 0.469403, l1: 0.479312, l2: 0.475861, l3: 0.480692, l4: 0.471351, l5: 0.473913, l6: 0.477763\n",
      "\n",
      "[epoch:   1/500, batch:  3924/ 4800, ite: 109] train loss: 4.152569, tar: 0.590488 \n",
      "l0: 0.573125, l1: 0.590911, l2: 0.586195, l3: 0.597506, l4: 0.611477, l5: 0.577025, l6: 0.601129\n",
      "\n",
      "[epoch:   1/500, batch:  3960/ 4800, ite: 110] train loss: 4.152431, tar: 0.590330 \n",
      "l0: 0.497099, l1: 0.525904, l2: 0.513209, l3: 0.528277, l4: 0.528739, l5: 0.504814, l6: 0.508665\n",
      "\n",
      "[epoch:   1/500, batch:  3996/ 4800, ite: 111] train loss: 4.147515, tar: 0.589491 \n",
      "l0: 0.495275, l1: 0.512922, l2: 0.509702, l3: 0.507419, l4: 0.497626, l5: 0.500636, l6: 0.504297\n",
      "\n",
      "[epoch:   1/500, batch:  4032/ 4800, ite: 112] train loss: 4.141983, tar: 0.588649 \n",
      "l0: 0.513706, l1: 0.544946, l2: 0.540055, l3: 0.544403, l4: 0.523668, l5: 0.525943, l6: 0.522798\n",
      "\n",
      "[epoch:   1/500, batch:  4068/ 4800, ite: 113] train loss: 4.138208, tar: 0.587986 \n",
      "l0: 0.545816, l1: 0.595586, l2: 0.581621, l3: 0.598849, l4: 0.583602, l5: 0.539957, l6: 0.555516\n",
      "\n",
      "[epoch:   1/500, batch:  4104/ 4800, ite: 114] train loss: 4.137004, tar: 0.587616 \n",
      "l0: 0.495336, l1: 0.462536, l2: 0.463002, l3: 0.480673, l4: 0.504723, l5: 0.491843, l6: 0.523679\n",
      "\n",
      "[epoch:   1/500, batch:  4140/ 4800, ite: 115] train loss: 4.130785, tar: 0.586814 \n",
      "l0: 0.535050, l1: 0.567618, l2: 0.559380, l3: 0.558283, l4: 0.547719, l5: 0.540308, l6: 0.537000\n",
      "\n",
      "[epoch:   1/500, batch:  4176/ 4800, ite: 116] train loss: 4.128325, tar: 0.586367 \n",
      "l0: 0.613632, l1: 0.595664, l2: 0.612384, l3: 0.597413, l4: 0.597419, l5: 0.600430, l6: 0.686502\n",
      "\n",
      "[epoch:   1/500, batch:  4212/ 4800, ite: 117] train loss: 4.129821, tar: 0.586601 \n",
      "l0: 0.515176, l1: 0.510930, l2: 0.507436, l3: 0.513300, l4: 0.529662, l5: 0.508947, l6: 0.513499\n",
      "\n",
      "[epoch:   1/500, batch:  4248/ 4800, ite: 118] train loss: 4.125322, tar: 0.585995 \n",
      "l0: 0.486594, l1: 0.494782, l2: 0.488991, l3: 0.500404, l4: 0.491132, l5: 0.474950, l6: 0.474576\n",
      "\n",
      "[epoch:   1/500, batch:  4284/ 4800, ite: 119] train loss: 4.119323, tar: 0.585160 \n",
      "l0: 0.499174, l1: 0.509358, l2: 0.498291, l3: 0.505026, l4: 0.500125, l5: 0.490523, l6: 0.487667\n",
      "\n",
      "[epoch:   1/500, batch:  4320/ 4800, ite: 120] train loss: 4.114080, tar: 0.584443 \n",
      "l0: 0.510215, l1: 0.528709, l2: 0.514148, l3: 0.512774, l4: 0.506552, l5: 0.504299, l6: 0.522066\n",
      "\n",
      "[epoch:   1/500, batch:  4356/ 4800, ite: 121] train loss: 4.109821, tar: 0.583830 \n",
      "l0: 0.482342, l1: 0.485869, l2: 0.471208, l3: 0.480345, l4: 0.481352, l5: 0.476163, l6: 0.477394\n",
      "\n",
      "[epoch:   1/500, batch:  4392/ 4800, ite: 122] train loss: 4.103631, tar: 0.582998 \n",
      "l0: 0.455060, l1: 0.474819, l2: 0.464111, l3: 0.463174, l4: 0.450062, l5: 0.444788, l6: 0.452772\n",
      "\n",
      "[epoch:   1/500, batch:  4428/ 4800, ite: 123] train loss: 4.096323, tar: 0.581958 \n",
      "l0: 0.525686, l1: 0.507725, l2: 0.507374, l3: 0.511642, l4: 0.529263, l5: 0.528720, l6: 0.550172\n",
      "\n",
      "[epoch:   1/500, batch:  4464/ 4800, ite: 124] train loss: 4.092810, tar: 0.581504 \n",
      "l0: 0.504259, l1: 0.489295, l2: 0.491060, l3: 0.493490, l4: 0.507687, l5: 0.501647, l6: 0.517249\n",
      "\n",
      "[epoch:   1/500, batch:  4500/ 4800, ite: 125] train loss: 4.088105, tar: 0.580886 \n",
      "l0: 0.482840, l1: 0.483358, l2: 0.489954, l3: 0.482919, l4: 0.490368, l5: 0.496384, l6: 0.496183\n",
      "\n",
      "[epoch:   1/500, batch:  4536/ 4800, ite: 126] train loss: 4.082819, tar: 0.580108 \n",
      "l0: 0.561184, l1: 0.543737, l2: 0.550099, l3: 0.564344, l4: 0.584243, l5: 0.563225, l6: 0.576534\n",
      "\n",
      "[epoch:   1/500, batch:  4572/ 4800, ite: 127] train loss: 4.081720, tar: 0.579959 \n",
      "l0: 0.457883, l1: 0.464825, l2: 0.458983, l3: 0.461011, l4: 0.468201, l5: 0.456702, l6: 0.468691\n",
      "\n",
      "[epoch:   1/500, batch:  4608/ 4800, ite: 128] train loss: 4.075115, tar: 0.579005 \n",
      "l0: 0.564263, l1: 0.548354, l2: 0.554737, l3: 0.562798, l4: 0.582933, l5: 0.575693, l6: 0.599799\n",
      "\n",
      "[epoch:   1/500, batch:  4644/ 4800, ite: 129] train loss: 4.074444, tar: 0.578891 \n",
      "l0: 0.495008, l1: 0.497736, l2: 0.498015, l3: 0.499286, l4: 0.511428, l5: 0.495539, l6: 0.511237\n",
      "\n",
      "[epoch:   1/500, batch:  4680/ 4800, ite: 130] train loss: 4.070089, tar: 0.578246 \n",
      "l0: 0.528746, l1: 0.500489, l2: 0.529298, l3: 0.531212, l4: 0.533837, l5: 0.526864, l6: 0.535528\n",
      "\n",
      "[epoch:   1/500, batch:  4716/ 4800, ite: 131] train loss: 4.067157, tar: 0.577868 \n",
      "l0: 0.467910, l1: 0.446854, l2: 0.469510, l3: 0.462189, l4: 0.465841, l5: 0.463499, l6: 0.464844\n",
      "\n",
      "[epoch:   1/500, batch:  4752/ 4800, ite: 132] train loss: 4.060895, tar: 0.577035 \n",
      "l0: 0.469689, l1: 0.461106, l2: 0.474143, l3: 0.470630, l4: 0.483654, l5: 0.469711, l6: 0.465536\n",
      "\n",
      "[epoch:   1/500, batch:  4788/ 4800, ite: 133] train loss: 4.055133, tar: 0.576228 \n",
      "l0: 0.427707, l1: 0.415731, l2: 0.428954, l3: 0.423112, l4: 0.440525, l5: 0.426612, l6: 0.417411\n",
      "\n",
      "[epoch:   1/500, batch:  4824/ 4800, ite: 134] train loss: 4.047110, tar: 0.575119 \n",
      "l0: 0.571763, l1: 0.564505, l2: 0.552813, l3: 0.560395, l4: 0.562599, l5: 0.615970, l6: 0.627385\n",
      "\n",
      "[epoch:   2/500, batch:    36/ 4800, ite: 135] train loss: 4.047172, tar: 0.575095 \n",
      "l0: 0.543175, l1: 0.533629, l2: 0.529432, l3: 0.536118, l4: 0.542516, l5: 0.578867, l6: 0.558775\n",
      "\n",
      "[epoch:   2/500, batch:    72/ 4800, ite: 136] train loss: 4.045520, tar: 0.574860 \n",
      "l0: 0.492948, l1: 0.466778, l2: 0.476680, l3: 0.482479, l4: 0.489821, l5: 0.487764, l6: 0.513065\n",
      "\n",
      "[epoch:   2/500, batch:   108/ 4800, ite: 137] train loss: 4.040877, tar: 0.574262 \n",
      "l0: 0.458116, l1: 0.427754, l2: 0.438281, l3: 0.441603, l4: 0.438152, l5: 0.450311, l6: 0.522856\n",
      "\n",
      "[epoch:   2/500, batch:   144/ 4800, ite: 138] train loss: 4.034618, tar: 0.573420 \n",
      "l0: 0.490908, l1: 0.485973, l2: 0.494394, l3: 0.494665, l4: 0.481491, l5: 0.505269, l6: 0.529287\n",
      "\n",
      "[epoch:   2/500, batch:   180/ 4800, ite: 139] train loss: 4.030643, tar: 0.572827 \n",
      "l0: 0.501450, l1: 0.514957, l2: 0.524516, l3: 0.521723, l4: 0.504914, l5: 0.495456, l6: 0.494609\n",
      "\n",
      "[epoch:   2/500, batch:   216/ 4800, ite: 140] train loss: 4.027264, tar: 0.572317 \n",
      "l0: 0.481885, l1: 0.478301, l2: 0.485317, l3: 0.487004, l4: 0.486277, l5: 0.487270, l6: 0.490811\n",
      "\n",
      "[epoch:   2/500, batch:   252/ 4800, ite: 141] train loss: 4.022792, tar: 0.571676 \n",
      "l0: 0.458798, l1: 0.466456, l2: 0.466037, l3: 0.470865, l4: 0.462601, l5: 0.475481, l6: 0.476635\n",
      "\n",
      "[epoch:   2/500, batch:   288/ 4800, ite: 142] train loss: 4.017540, tar: 0.570881 \n",
      "l0: 0.495473, l1: 0.470169, l2: 0.483659, l3: 0.484318, l4: 0.478752, l5: 0.483122, l6: 0.533357\n",
      "\n",
      "[epoch:   2/500, batch:   324/ 4800, ite: 143] train loss: 4.013422, tar: 0.570353 \n",
      "l0: 0.582861, l1: 0.565703, l2: 0.579205, l3: 0.581126, l4: 0.573939, l5: 0.601048, l6: 0.616613\n",
      "\n",
      "[epoch:   2/500, batch:   360/ 4800, ite: 144] train loss: 4.014028, tar: 0.570440 \n",
      "l0: 0.483864, l1: 0.472906, l2: 0.476812, l3: 0.482423, l4: 0.487857, l5: 0.500679, l6: 0.513524\n",
      "\n",
      "[epoch:   2/500, batch:   396/ 4800, ite: 145] train loss: 4.009918, tar: 0.569843 \n",
      "l0: 0.519944, l1: 0.502304, l2: 0.513179, l3: 0.512391, l4: 0.513095, l5: 0.534660, l6: 0.561505\n",
      "\n",
      "[epoch:   2/500, batch:   432/ 4800, ite: 146] train loss: 4.007501, tar: 0.569501 \n",
      "l0: 0.518952, l1: 0.666436, l2: 0.511011, l3: 0.564220, l4: 0.522046, l5: 0.521947, l6: 0.585901\n",
      "\n",
      "[epoch:   2/500, batch:   468/ 4800, ite: 147] train loss: 4.006705, tar: 0.569158 \n",
      "l0: 0.489047, l1: 0.462634, l2: 0.475657, l3: 0.481130, l4: 0.485940, l5: 0.492407, l6: 0.502376\n",
      "\n",
      "[epoch:   2/500, batch:   504/ 4800, ite: 148] train loss: 4.002533, tar: 0.568616 \n",
      "l0: 0.496753, l1: 0.466143, l2: 0.490590, l3: 0.499118, l4: 0.502217, l5: 0.504497, l6: 0.497701\n",
      "\n",
      "[epoch:   2/500, batch:   540/ 4800, ite: 149] train loss: 3.998872, tar: 0.568134 \n",
      "l0: 0.436383, l1: 0.416568, l2: 0.439535, l3: 0.442571, l4: 0.443730, l5: 0.438268, l6: 0.440486\n",
      "\n",
      "[epoch:   2/500, batch:   576/ 4800, ite: 150] train loss: 3.992597, tar: 0.567256 \n",
      "l0: 0.541946, l1: 0.548015, l2: 0.557899, l3: 0.550292, l4: 0.550251, l5: 0.534348, l6: 0.592134\n",
      "\n",
      "[epoch:   2/500, batch:   612/ 4800, ite: 151] train loss: 3.991817, tar: 0.567088 \n",
      "l0: 0.522364, l1: 0.508304, l2: 0.502144, l3: 0.498199, l4: 0.500637, l5: 0.520160, l6: 0.582259\n",
      "\n",
      "[epoch:   2/500, batch:   648/ 4800, ite: 152] train loss: 3.989463, tar: 0.566794 \n",
      "l0: 0.445694, l1: 0.443821, l2: 0.438530, l3: 0.442244, l4: 0.443595, l5: 0.458056, l6: 0.442069\n",
      "\n",
      "[epoch:   2/500, batch:   684/ 4800, ite: 153] train loss: 3.983742, tar: 0.566002 \n",
      "l0: 0.526312, l1: 0.524615, l2: 0.516346, l3: 0.521920, l4: 0.535760, l5: 0.555654, l6: 0.541552\n",
      "\n",
      "[epoch:   2/500, batch:   720/ 4800, ite: 154] train loss: 3.982043, tar: 0.565745 \n",
      "l0: 0.474623, l1: 0.453662, l2: 0.468542, l3: 0.461590, l4: 0.450579, l5: 0.445501, l6: 0.561960\n",
      "\n",
      "[epoch:   2/500, batch:   756/ 4800, ite: 155] train loss: 3.977749, tar: 0.565157 \n",
      "l0: 0.462761, l1: 0.439334, l2: 0.451670, l3: 0.451870, l4: 0.448546, l5: 0.451856, l6: 0.503122\n",
      "\n",
      "[epoch:   2/500, batch:   792/ 4800, ite: 156] train loss: 3.972822, tar: 0.564500 \n",
      "l0: 0.475904, l1: 0.467209, l2: 0.469018, l3: 0.467562, l4: 0.471355, l5: 0.473782, l6: 0.488693\n",
      "\n",
      "[epoch:   2/500, batch:   828/ 4800, ite: 157] train loss: 3.968623, tar: 0.563936 \n",
      "l0: 0.467078, l1: 0.453846, l2: 0.450104, l3: 0.456270, l4: 0.467514, l5: 0.463210, l6: 0.484423\n",
      "\n",
      "[epoch:   2/500, batch:   864/ 4800, ite: 158] train loss: 3.964027, tar: 0.563323 \n",
      "l0: 0.537938, l1: 0.531235, l2: 0.526356, l3: 0.537587, l4: 0.538176, l5: 0.522212, l6: 0.570231\n",
      "\n",
      "[epoch:   2/500, batch:   900/ 4800, ite: 159] train loss: 3.962767, tar: 0.563163 \n",
      "l0: 0.536060, l1: 0.517707, l2: 0.524169, l3: 0.536474, l4: 0.537560, l5: 0.538240, l6: 0.562926\n",
      "\n",
      "[epoch:   2/500, batch:   936/ 4800, ite: 160] train loss: 3.961457, tar: 0.562994 \n",
      "l0: 0.445428, l1: 0.445295, l2: 0.445791, l3: 0.450019, l4: 0.450286, l5: 0.447442, l6: 0.453644\n",
      "\n",
      "[epoch:   2/500, batch:   972/ 4800, ite: 161] train loss: 3.956341, tar: 0.562264 \n",
      "l0: 0.456235, l1: 0.442185, l2: 0.451743, l3: 0.456363, l4: 0.459498, l5: 0.459592, l6: 0.482842\n",
      "\n",
      "[epoch:   2/500, batch:  1008/ 4800, ite: 162] train loss: 3.951725, tar: 0.561609 \n",
      "l0: 0.470067, l1: 0.467612, l2: 0.471617, l3: 0.476333, l4: 0.471544, l5: 0.471851, l6: 0.477752\n",
      "\n",
      "[epoch:   2/500, batch:  1044/ 4800, ite: 163] train loss: 3.947768, tar: 0.561048 \n",
      "l0: 0.447193, l1: 0.446256, l2: 0.445419, l3: 0.441475, l4: 0.451332, l5: 0.442945, l6: 0.463286\n",
      "\n",
      "[epoch:   2/500, batch:  1080/ 4800, ite: 164] train loss: 3.942829, tar: 0.560353 \n",
      "l0: 0.403457, l1: 0.401368, l2: 0.408292, l3: 0.405554, l4: 0.399488, l5: 0.401867, l6: 0.395637\n",
      "\n",
      "[epoch:   2/500, batch:  1116/ 4800, ite: 165] train loss: 3.935998, tar: 0.559402 \n",
      "l0: 0.475332, l1: 0.449791, l2: 0.458116, l3: 0.461594, l4: 0.473950, l5: 0.474392, l6: 0.498162\n",
      "\n",
      "[epoch:   2/500, batch:  1152/ 4800, ite: 166] train loss: 3.932114, tar: 0.558896 \n",
      "l0: 0.492324, l1: 0.466351, l2: 0.478638, l3: 0.475818, l4: 0.486109, l5: 0.491559, l6: 0.524071\n",
      "\n",
      "[epoch:   2/500, batch:  1188/ 4800, ite: 167] train loss: 3.929017, tar: 0.558497 \n",
      "l0: 0.404078, l1: 0.396132, l2: 0.407148, l3: 0.409124, l4: 0.406534, l5: 0.407175, l6: 0.396380\n",
      "\n",
      "[epoch:   2/500, batch:  1224/ 4800, ite: 168] train loss: 3.922455, tar: 0.557578 \n",
      "l0: 0.575272, l1: 0.574834, l2: 0.592650, l3: 0.582045, l4: 0.569435, l5: 0.586157, l6: 0.582755\n",
      "\n",
      "[epoch:   2/500, batch:  1260/ 4800, ite: 169] train loss: 3.923288, tar: 0.557683 \n",
      "l0: 0.448945, l1: 0.443209, l2: 0.456351, l3: 0.455431, l4: 0.444710, l5: 0.445595, l6: 0.453226\n",
      "\n",
      "[epoch:   2/500, batch:  1296/ 4800, ite: 170] train loss: 3.918724, tar: 0.557043 \n",
      "l0: 0.426668, l1: 0.427689, l2: 0.425550, l3: 0.424368, l4: 0.426999, l5: 0.445138, l6: 0.458394\n",
      "\n",
      "[epoch:   2/500, batch:  1332/ 4800, ite: 171] train loss: 3.913555, tar: 0.556281 \n",
      "l0: 0.462150, l1: 0.456514, l2: 0.465010, l3: 0.466597, l4: 0.460998, l5: 0.463481, l6: 0.471474\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"---start training...\")\n",
    "\n",
    "for epoch in range(0, epoch_num):\n",
    "    net.train()\n",
    "\n",
    "    for i, data in enumerate(salobj_dataloader):\n",
    "        ite_num = ite_num + 1\n",
    "        ite_num4val = ite_num4val + 1\n",
    "\n",
    "        inputs, labels = data['image'], data['label']\n",
    "\n",
    "        inputs = inputs.type(torch.FloatTensor)\n",
    "        labels = labels.type(torch.FloatTensor)\n",
    "\n",
    "        # wrap them in Variable\n",
    "        if torch.cuda.is_available():\n",
    "            inputs_v, labels_v = Variable(inputs.cuda(), requires_grad=False), Variable(labels.cuda(),\n",
    "                                                                                        requires_grad=False)\n",
    "        else:\n",
    "            inputs_v, labels_v = Variable(inputs, requires_grad=False), Variable(labels, requires_grad=False)\n",
    "\n",
    "        # y zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        d0, d1, d2, d3, d4, d5, d6= net(inputs_v)\n",
    "        loss2, loss = muti_bce_loss_fusion(d0, d1, d2, d3, d4,d5, d6, labels_v)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # # print statistics\n",
    "        running_loss += loss.data\n",
    "        running_tar_loss += loss2.data\n",
    "\n",
    "        # del temporary outputs and loss\n",
    "        del d0, d1, d2, d3, d4, loss2, loss\n",
    "\n",
    "        print(\"[epoch: %3d/%3d, batch: %5d/%5d, ite: %d] train loss: %3f, tar: %3f \" % (\n",
    "        epoch + 1, epoch_num, (i + 1) * batch_size_train, train_num, ite_num, running_loss / ite_num4val, running_tar_loss / ite_num4val))\n",
    "\n",
    "\n",
    "    if (epoch+1) % save_epoch== 0:\n",
    "\n",
    "        torch.save(net.state_dict(), model_dir + model_name+\"_bce_itr_%d_train_%3f_tar_%3f.pth\" % (ite_num, running_loss / ite_num4val, running_tar_loss / ite_num4val))\n",
    "        Loss_list.append(running_loss / ite_num4val)\n",
    "        running_loss = 0.0\n",
    "        running_tar_loss = 0.0\n",
    "        net.train()  # resume train\n",
    "        ite_num4val = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## plot loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = range(0, len(Loss_list))\n",
    "y = Loss_list\n",
    "plt.plot(x, y, '.-')\n",
    "plt.xlabel('Test loss vs. ite_num')\n",
    "plt.ylabel('Test loss')\n",
    "plt.savefig(\"loss/loss_{}.png\".format(str(epoch+1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myconda",
   "language": "python",
   "name": "myconda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
